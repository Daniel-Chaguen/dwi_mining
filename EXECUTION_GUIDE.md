# üìñ Gu√≠a de Ejecuci√≥n del Proyecto

Esta gu√≠a detalla paso a paso c√≥mo ejecutar el proyecto de clasificaci√≥n de trastornos neuropsiqui√°tricos usando im√°genes DWI.

---

## üîß Configuraci√≥n del Entorno

### Paso 1: Instalar Dependencias

```bash
# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar paquetes base
pip install numpy pandas matplotlib seaborn jupyter
pip install scikit-learn xgboost

# Instalar herramientas de neuroimagen
pip install nibabel dipy

# Instalar PyTorch (ajustar seg√∫n tu hardware)
# CPU:
pip install torch torchvision

# GPU (CUDA 11.8):
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Paso 2: Verificar Instalaci√≥n

```python
import torch
import nibabel as nib
import dipy
import sklearn

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Nibabel version: {nib.__version__}")
print(f"DIPY version: {dipy.__version__}")
```

---

## üì• Descarga de Datos

### Opci√≥n 1: Google Colab (Recomendada)

1. Abre `scripts/data_download.ipynb` en Google Colab
2. Monta tu Google Drive:
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```
3. Actualiza la variable `BASE_PATH` con tu ruta de Google Drive
4. Ejecuta todas las celdas para descargar el dataset completo

### Opci√≥n 2: Descarga Manual

1. Visita [OpenNeuro](https://openneuro.org/datasets/ds000030/versions/1.0.0)
2. Descarga el dataset completo (‚ö†Ô∏è ~70 GB)
3. Organiza los archivos seg√∫n estructura BIDS:
   ```
   data/
   ‚îú‚îÄ‚îÄ sub-10159/
   ‚îÇ   ‚îî‚îÄ‚îÄ dwi/
   ‚îÇ       ‚îú‚îÄ‚îÄ sub-10159_dwi.nii.gz
   ‚îÇ       ‚îú‚îÄ‚îÄ sub-10159_dwi.bval
   ‚îÇ       ‚îî‚îÄ‚îÄ sub-10159_dwi.bvec
   ‚îú‚îÄ‚îÄ sub-10171/
   ...
   ```

---

## üöÄ Ejecuci√≥n del Pipeline Completo

### Fase 1 y 2: Entendimiento de Datos

```bash
# Ejecutar notebook de an√°lisis exploratorio
jupyter notebook mineria_fase2.ipynb
```

**Salidas esperadas:**
- Distribuci√≥n de diagn√≥sticos
- Estad√≠sticas de metadatos DWI
- Visualizaci√≥n 3D de vol√∫menes de ejemplo

---

### Fase 3: Preparaci√≥n de Datos

#### A. Generar Caracter√≠sticas FA (Bloques)

```bash
jupyter notebook mineria_fase3.ipynb
```

**Proceso:**
1. Carga de vol√∫menes DWI (96√ó96√ó50√ó64)
2. C√°lculo del modelo de tensor de difusi√≥n (DTI)
3. Extracci√≥n de Anisotrop√≠a Fraccional (FA)
4. Divisi√≥n en bloques 8√ó8√ó8 (512 features)
5. Guardado en `out/dwi_block_features_for_svm.csv`

**‚è±Ô∏è Tiempo estimado:** 2-3 horas (200 sujetos)

#### B. Preprocesar Vol√∫menes para CNN

El mismo notebook `mineria_fase3.ipynb` tambi√©n:
1. Filtra vol√∫menes por dimensiones (96√ó96√ó50)
2. Normaliza con z-score por sujeto
3. Guarda bloques pickle en `out/data/block_*.pkl`
4. Genera `manifest.csv` con √≠ndices

---

### Fase 4: Modelado

#### Modelo 1: SVM con Caracter√≠sticas FA

```python
# En mineria_fase4.ipynb (secci√≥n "Modelos Cl√°sicos")
# Ejecutar celdas de entrenamiento SVM:

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# Pipeline: Scaling ‚Üí SVM
pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(class_weight="balanced"))
])

# Grid Search
param_grid = {
    "svm__kernel": ["rbf", "linear"],
    "svm__C": [0.1, 1, 10, 100],
    "svm__gamma": ["scale", "auto", 0.01, 0.1]
}

grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring="balanced_accuracy")
grid_search.fit(X_train, y_train)

# Guardar mejor modelo
best_model = grid_search.best_estimator_
```

**Salidas:**
- `out/svm_model.pkl`
- Mejores hiperpar√°metros
- Balanced Accuracy en validaci√≥n cruzada



#### Modelo 2: CNN 3D

```python
# En mineria_fase4.ipynb (secci√≥n "CNN 3D")
# 1. Cargar vol√∫menes preprocesados
volumes, labels = load_all_blocks_to_ram()

# 2. Instanciar modelo
model = ImprovedCNN3D(in_channels=64, num_classes=2)

# 3. Entrenar
train(
    volumes=volumes,
    labels=labels,
    out_dir=OUT_FOLDER,
    instance_model=model,
    model_name="v2_cnn"
)
```

**Configuraci√≥n de Entrenamiento:**
- Batch size: 8
- Epochs: 150 (con early stopping por weighted accuracy)
- Optimizer: Adam (lr=1e-4)
- Loss: CrossEntropyLoss
- Regularizaci√≥n: Dropout (0.1-0.2), BatchNorm

**‚è±Ô∏è Tiempo de entrenamiento:**
- CPU: ~10-15 horas
- GPU (RTX 3070): ~2-3 horas

**Salidas:**
- `out/best_v2_cnn.pth` (mejor checkpoint)
- `out/v2_cnn.pth` (√∫ltima √©poca)
- `out/v2_cnn.csv` (hist√≥rico de entrenamiento)
- `out/v2_cnn_training.png` (curvas de loss/accuracy)

---

### Fase 5: Evaluaci√≥n

```bash
jupyter notebook mineria_fase5.ipynb
```

**An√°lisis incluidos:**

1. **M√©tricas Cuantitativas**
   - Confusion matrix
   - Classification report (precision, recall, F1)
   - Balanced accuracy

2. **Visualizaciones t-SNE**
   - Espacio latente del SVM (features FA)
   - Embeddings de la CNN (features de block4)
   - Comparaci√≥n etiquetas reales vs predicciones

3. **An√°lisis de Activaciones (CNN)**
   - Mapas de activaci√≥n por capa
   - Comparaci√≥n input original vs features aprendidos
   - Visualizaci√≥n de transformaci√≥n progresiva

4. **An√°lisis de Errores**
   - Ejemplos correctamente clasificados
   - Ejemplos mal clasificados
   - Distribuci√≥n de confianza (probabilidades)

**Salidas:**
- Gr√°ficas de matrices de confusi√≥n
- t-SNE plots (PNG)
- Mapas de activaci√≥n (PNG)
- Reporte de m√©tricas (texto)

---

